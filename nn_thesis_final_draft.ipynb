{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjDIe40KyzEX"
      },
      "source": [
        "# 1.Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAQ3g3yYy0CO",
        "outputId": "c3694be7-b902-44e7-ec0a-fb6d1ddebbc7"
      },
      "outputs": [],
      "source": [
        "#Install necessary libraries\n",
        "!pip install audiomentations\n",
        "!pip install cylimiter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-hEAS2QxyKfO"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, fbeta_score, make_scorer\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten,MaxPool1D, Conv1D\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import os\n",
        "import scipy\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from config import Config\n",
        "from audiomentations import Compose, Limiter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V0sS8V8Lp2Ct"
      },
      "outputs": [],
      "source": [
        "#Initialize config\n",
        "conf = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpEa4DhKLkif"
      },
      "source": [
        "# 2. Signal-Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM-Nq4PNjQbm"
      },
      "source": [
        "## 2.1 Load audio signals and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "alGpEz5tp2Cv"
      },
      "outputs": [],
      "source": [
        "# Load audio files\n",
        "\n",
        "signal_names = ['clips_lakers', 'lakers_okc', 'okc_denver', 'portland_gsw', 'rockets_knicks', 'sixers_lakers', 'memphis_okc', 'bucks_toronto', 'gsw_cavs', 'portland_lakers']\n",
        "signals = {}\n",
        "for sig in signal_names:\n",
        "    sig_ = sig + '.wav'\n",
        "    audio_path = os.path.join('/wavfiles', sig_)\n",
        "    signal, sr = librosa.load(audio_path, sr=conf.sr)\n",
        "    signals[sig] = signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Zx-rn3r13Rx0"
      },
      "outputs": [],
      "source": [
        "#Load CSV files with labels in a dictionary\n",
        "df_dict = {}\n",
        "for audio in signals:\n",
        "  df_dict[audio] = pd.read_csv('/csv_files/'+audio+'.csv')\n",
        "  df_dict[audio]['LABEL'] = df_dict[audio]['LABEL'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iwwFqFzDHVN"
      },
      "source": [
        "## 2.2 Keep only annotated audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BhfCZhbuCUyV"
      },
      "outputs": [],
      "source": [
        "# Create a new dictionary to store the filtered audio data\n",
        "filtered_signals = {}\n",
        "\n",
        "# Iterate through each audio and its corresponding DataFrame\n",
        "for audio_name, signal in signals.items():\n",
        "    # Get the DataFrame for the current audio\n",
        "    df = df_dict[audio_name]\n",
        "\n",
        "    # Initialize an empty list to store the filtered audio samples\n",
        "    filtered_audio_samples = []\n",
        "\n",
        "    # Iterate through each row in the DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        # Extract the starting time and duration from the DataFrame\n",
        "        starting_time = row['TIME']\n",
        "        duration = row['DURATION']\n",
        "\n",
        "        # Convert time to sample indices\n",
        "        start_sample = int(librosa.time_to_samples(starting_time, sr=conf.sr))\n",
        "        end_sample = start_sample + int(librosa.time_to_samples(duration, sr=conf.sr))\n",
        "\n",
        "        # Extract the samples corresponding to the annotated time\n",
        "        annotated_audio = signal[start_sample:end_sample]\n",
        "\n",
        "        # Append the annotated audio to the list\n",
        "        filtered_audio_samples.extend(annotated_audio)\n",
        "\n",
        "    # Store the filtered audio samples in the new dictionary\n",
        "    filtered_signals[audio_name] = np.array(filtered_audio_samples)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qke5QH8SL4Jz"
      },
      "source": [
        "# 3. Model Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOZm-9ObEQJT"
      },
      "source": [
        "## 3.1 Data Augmentation(Dynamic Range Compression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "4catQwQPavuW"
      },
      "outputs": [],
      "source": [
        "def aug(sig_dict):\n",
        "\n",
        "  augment = Compose([Limiter()])\n",
        "  aug_signals = {}\n",
        "  for audio in sig_dict:\n",
        "\n",
        "    aug_signals[audio] = np.array(augment(samples=sig_dict[audio], sample_rate = conf.sr))\n",
        "\n",
        "  return aug_signals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYk3ZCUYp2C0"
      },
      "source": [
        "## 3.2 Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2.1 Features_extraction(except mel_spectrograms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "_7Xjo2Y9p2C1"
      },
      "outputs": [],
      "source": [
        "def feat_extr(sig, lp_order=12, n_mfccs=13):\n",
        "\n",
        "    mfccs = {}\n",
        "    centroids = {}\n",
        "    zcrs = {}\n",
        "    chroma = {}\n",
        "    flatness = {}\n",
        "    rolloff = {}\n",
        "    rms = {}\n",
        "    contrast = {}\n",
        "    bandwidth = {}\n",
        "\n",
        "    lp_all = {}\n",
        "    lpc_dict = {}\n",
        "    lpcc_dict = {}\n",
        "\n",
        "    #Dictionary containing features EXCEPT LPC and LPCC\n",
        "    features = {}\n",
        "\n",
        "    feat_all = {}\n",
        "\n",
        "    for audio in sig:\n",
        "\n",
        "        mfcc = librosa.feature.mfcc(y=sig[audio], sr=conf.sr, n_fft=conf.nfft, hop_length=conf.hop, center=False,n_mfcc=n_mfccs)\n",
        "        mfccs[audio] = mfcc\n",
        "\n",
        "        cent = librosa.feature.spectral_centroid(y=sig[audio], sr=conf.sr, n_fft=conf.nfft, hop_length=conf.hop, center=False)\n",
        "        centroids[audio] = cent\n",
        "\n",
        "        zcr = librosa.feature.zero_crossing_rate(y=sig[audio],frame_length=conf.nfft, hop_length=conf.hop, center=False)\n",
        "        zcrs[audio] = zcr\n",
        "\n",
        "        chromagram = librosa.feature.chroma_stft(y=sig[audio], sr=conf.sr, n_fft=conf.nfft, hop_length=conf.hop, center=False)\n",
        "        chroma[audio] = chromagram\n",
        "\n",
        "        flt = librosa.feature.spectral_flatness(y=sig[audio], n_fft=conf.nfft, hop_length=conf.hop, center=False)\n",
        "        flatness[audio] = flt\n",
        "\n",
        "        roll = librosa.feature.spectral_rolloff(y=sig[audio], sr=conf.sr, n_fft=conf.nfft, hop_length=conf.hop, center=False)\n",
        "        rolloff[audio] = roll\n",
        "\n",
        "        root_mean_square = librosa.feature.rms(y=sig[audio], frame_length=conf.nfft, hop_length=conf.hop, center=False)\n",
        "        rms[audio] = root_mean_square\n",
        "\n",
        "        contr = librosa.feature.spectral_contrast(y=sig[audio], sr=conf.sr, n_fft=conf.nfft, hop_length=conf.hop, center=False)\n",
        "        contrast[audio] = contr\n",
        "\n",
        "        spec_band = librosa.feature.spectral_bandwidth(y=sig[audio], sr=conf.sr, n_fft = conf.nfft, hop_length=conf.hop, center=False)\n",
        "        bandwidth[audio] = spec_band\n",
        "\n",
        "        #Concatenate features for each audio vertically and invert each numPy array of features\n",
        "        features[audio] = np.concatenate((mfccs[audio], zcrs[audio], centroids[audio], chroma[audio], flatness[audio], rolloff[audio], rms[audio], contrast[audio], bandwidth[audio]), axis=0).T\n",
        "\n",
        "\n",
        "        #Linear Predictive Coding(LPC) and Linear Prediction Cepstral Coefficients features extraction\n",
        "\n",
        "        #Step 1: Compute Linear Predictive Coding(LPC)\n",
        "        lpc_arr = []\n",
        "        lpcc_coeffs = []\n",
        "\n",
        "        frames = librosa.util.frame(sig[audio], frame_length=conf.nfft, hop_length=conf.hop).T\n",
        "        for frame in frames:\n",
        "            lpc = librosa.lpc(frame, order=lp_order)\n",
        "            lpc_arr.append(lpc)\n",
        "\n",
        "        lpc_arr = np.array(lpc_arr)\n",
        "        lpc_dict[audio] = lpc_arr\n",
        "\n",
        "        #Step 2: Linear Prediction Cepstral Coefficients(LPCC)\n",
        "        for lpc in lpc_arr:\n",
        "            lpcc = np.zeros(lp_order)\n",
        "            lpcc[0] = np.log(lp_order-1)\n",
        "\n",
        "            #Recursive formula for LPCC computation\n",
        "            for m in range(1, lp_order):\n",
        "                lpcc[m] = lpc[m]\n",
        "                for k in range(1, m):\n",
        "                    lpcc[m] += (k / m) * lpc[m - k] * lpcc[k]\n",
        "            lpcc_coeffs.append(lpcc)\n",
        "\n",
        "        lpcc_coeffs = np.array(lpcc_coeffs)\n",
        "        lpcc_dict[audio] = lpcc_coeffs\n",
        "\n",
        "        #Concatenate LPC and LPCC horizontally\n",
        "        lp_all[audio] = np.concatenate((lpc_dict[audio], lpcc_dict[audio]), axis=1)\n",
        "\n",
        "        #Concatenate all features horizontally to get final feature vector\n",
        "        feat_all[audio] = np.concatenate((features[audio], lp_all[audio]), axis=1)\n",
        "\n",
        "    return feat_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2.2 Mel_spectrograms feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "esp9iNfiZ72q"
      },
      "outputs": [],
      "source": [
        "#Mel spectrograms computation\n",
        "def generate_mel_spec(sig, mels=128): \n",
        "\n",
        "  mels_dict = {}\n",
        "  for audio in sig:\n",
        "    mels_dict[audio] = librosa.feature.melspectrogram(y=sig[audio], sr=conf.sr, n_fft=conf.nfft, hop_length=conf.hop, center=False, n_mels=mels).T\n",
        "\n",
        "  return mels_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHG5FMQzL-ID"
      },
      "source": [
        "## 3.3 Frame labelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "-aJvj_cZjbAT"
      },
      "outputs": [],
      "source": [
        "def new_frame_labelling(features_dict, df_dict):\n",
        "\n",
        "  labels = {}\n",
        "  for audio in features_dict:\n",
        "    #Initialize start_time on each audio\n",
        "    start_time = 0\n",
        "\n",
        "    labels[audio] = np.zeros(features_dict[audio].shape[0])\n",
        "    for i, row in df_dict[audio].iterrows():\n",
        "\n",
        "      end_time = start_time + row['DURATION']\n",
        "      label = row[\"LABEL\"]\n",
        "\n",
        "    # Convert start and end times from seconds to frame indices.\n",
        "      start_frame = int(start_time * conf.sr/conf.hop)\n",
        "      end_frame = int(end_time * conf.sr/conf.hop)\n",
        "\n",
        "      #Assign labels to frames\n",
        "      if label == 'cheering':\n",
        "        labels[audio][start_frame:end_frame] = 0\n",
        "\n",
        "      elif label == 'whistle':\n",
        "        labels[audio][start_frame:end_frame] = 1\n",
        "\n",
        "      elif label == 'air_horn':\n",
        "        labels[audio][start_frame:end_frame] = 2\n",
        "\n",
        "      elif label == 'speech':\n",
        "        labels[audio][start_frame:end_frame] = 3\n",
        "\n",
        "      elif label == 'boos':\n",
        "        labels[audio][start_frame:end_frame] = 4\n",
        "\n",
        "      elif label == 'other':\n",
        "        labels[audio][start_frame:end_frame] = 5\n",
        "\n",
        "      #end_time becomes start_time for next annotation\n",
        "      start_time = end_time\n",
        "\n",
        "  return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryCmoHbKMG6Z"
      },
      "source": [
        "## 3.4 Windows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNpXeVH9p2C4"
      },
      "source": [
        "### 3.4.1 Feature Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "UAc84gv5LvPW"
      },
      "outputs": [],
      "source": [
        "#ANN input shape : (#windows, #features * #frames_per_window)\n",
        "#cnn input shape : (#windows, #frames_per_window, #features, depth)\n",
        "\n",
        "def create_win(features_dict):\n",
        "\n",
        "  windows = {}\n",
        "\n",
        "  for audio in features_dict:\n",
        "\n",
        "    num_samples, num_features = features_dict[audio].shape\n",
        "    num_windows = (num_samples - conf.win_size) // conf.win_hop + 1\n",
        "\n",
        "    #Initialize dimensions of windows to fit input shape of Convolutional Neural Network\n",
        "    windows[audio] = np.zeros((num_windows, num_features, conf.win_size, 1))\n",
        "\n",
        "    #Iterate to pass frame features to each window, depending on window size\n",
        "    for i in range(num_windows):\n",
        "\n",
        "        start_idx = i * conf.win_hop\n",
        "        end_idx = start_idx + conf.win_size\n",
        "        windows[audio][i, :, :, 0] = features_dict[audio][start_idx:end_idx, :].T\n",
        "\n",
        "    #Fix window dimensions depending on type of Neural Network been utilized\n",
        "    if conf.nn == 'ann':\n",
        "        windows[audio] = windows[audio].reshape(windows[audio].shape[0], -1)\n",
        "\n",
        "  return windows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxg5N3lSp2C5"
      },
      "source": [
        "### 3.4.2 Window Stats(mean, variance, median)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "-Aa-PecZp2C6"
      },
      "outputs": [],
      "source": [
        "def win_stats(win):\n",
        "\n",
        "\n",
        "    means, vars, meds = [], [], []\n",
        "\n",
        "    #Check if conf.nn = 'ann' to reshape window in order to calculate stats\n",
        "    if conf.nn == 'ann':\n",
        "      win = win.reshape(win.shape[0], -1, conf.win_size)\n",
        "\n",
        "\n",
        "    #Iterate through each window and calculate mean, var and median for each feature\n",
        "    for i in range (win.shape[0]):\n",
        "        mean_per_feat = np.mean( win[i, :, :,],axis=1)\n",
        "        var_per_feat = np.var(win[i, :, :,], axis=1)\n",
        "        med_per_feat = np.median(win[i, :, :,], axis=1)\n",
        "\n",
        "        means.append(mean_per_feat)\n",
        "        vars.append(var_per_feat)\n",
        "        meds.append(med_per_feat)\n",
        "\n",
        "    #Concatenate stats horizontally\n",
        "    ovr = np.concatenate((means, vars, meds), axis=1)\n",
        "\n",
        "    #Check type of Neural Network to reshape ovr accordingly\n",
        "    if conf.nn == 'cnn':\n",
        "        reshaped_win = ovr.reshape(ovr.shape[0], ovr.shape[1], 1)\n",
        "    elif conf.nn == 'ann':\n",
        "        reshaped_win = ovr.reshape(ovr.shape[0], -1)\n",
        "\n",
        "\n",
        "    return reshaped_win"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYnIKA-jML6x"
      },
      "source": [
        "## 3.5 Windows labelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYPWqUKMMQLe"
      },
      "source": [
        "### 3.5.1 Majority Voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "wBDEVJ1sMR4F"
      },
      "outputs": [],
      "source": [
        "def majVot_win_lab(windows, frame_labels):\n",
        "\n",
        "  labels = {}\n",
        "\n",
        "  for audio in windows:\n",
        "\n",
        "      num_windows = windows[audio].shape[0]\n",
        "      labels[audio] = np.zeros(num_windows)\n",
        "\n",
        "\n",
        "      #Iterate over each window\n",
        "\n",
        "      for i in range(num_windows):\n",
        "          win_start = i * conf.win_hop\n",
        "          win_end = win_start + conf.win_size\n",
        "\n",
        "          #Count occurrences of each class in the window\n",
        "\n",
        "          class_counts = {'0': 0, '1': 0, '2': 0, '3': 0, '4': 0, '5': 0}\n",
        "\n",
        "          for frame_idx in range(win_start, win_end):\n",
        "              lab = str(int(frame_labels[audio][frame_idx]))\n",
        "              class_counts[lab] += 1\n",
        "\n",
        "          # Determine the majority class for the window\n",
        "          majority_class = max(class_counts, key=class_counts.get)\n",
        "\n",
        "          # Assign the majority class label to the entire window\n",
        "          window_label = int(majority_class)\n",
        "          labels[audio][i] = window_label\n",
        "\n",
        "  return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI6KY3I2MWzI"
      },
      "source": [
        "### 3.5.2 Weighted Voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "FqvmREPIMZeR"
      },
      "outputs": [],
      "source": [
        "def weigVot_win_lab(windows_dict, frame_labels_dict):\n",
        "\n",
        "    #Initialize weights for each label\n",
        "    labels = {}\n",
        "    label_weights = np.array([1, 2, 1, 1, 1, 1])\n",
        "\n",
        "    for audio in windows_dict:\n",
        "\n",
        "      num_windows = windows_dict[audio].shape[0]\n",
        "      labels[audio] = np.zeros(num_windows)\n",
        "\n",
        "      #Iterate over each window\n",
        "\n",
        "      for i in range(num_windows):\n",
        "          #Iterate through frame labels of each window\n",
        "          window_frame_labels = frame_labels_dict[audio][i*conf.win_hop : i* conf.win_hop + conf.win_size].astype(int)\n",
        "          #Count number of occurences of each class based on label_weights\n",
        "          label_counts = np.bincount(window_frame_labels, minlength=len(label_weights))\n",
        "          weighted_counts = label_counts * label_weights\n",
        "          #Assign index of max value of weighted_counts as label of corresponding window\n",
        "          labels[audio][i] = int(np.argmax(weighted_counts))\n",
        "\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WJyNfLNPH23"
      },
      "source": [
        "## 3.6 Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FntzKUpjPMbJ"
      },
      "source": [
        "### 3.6.1 Pearson Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "CrFVv-4HPGcq"
      },
      "outputs": [],
      "source": [
        "def pearsonr(X):\n",
        "\n",
        "    if X.ndim > 2:\n",
        "      X = X.reshape(X.shape[0], -1)\n",
        "\n",
        "    df_feat = pd.DataFrame(X)\n",
        "    corr = df_feat.corr()\n",
        "\n",
        "    #Convert DataFrame to numPy array\n",
        "    corr_mat = corr.values\n",
        "\n",
        "    # Initialize a list to store correlated feature pairs\n",
        "    correlated_pairs = []\n",
        "\n",
        "    # Define threshold for high correlation\n",
        "    correlation_threshold = 0.8\n",
        "\n",
        "    # Find highly correlated features\n",
        "    for i in range(corr.shape[0]):\n",
        "        for j in range(i + 1, corr.shape[0]):\n",
        "            if abs(corr_mat[i, j]) > correlation_threshold:\n",
        "                correlated_pairs.append((i, j))\n",
        "\n",
        "    correlated_pairs = np.array(correlated_pairs)\n",
        "\n",
        "    return correlated_pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkpMR9ZXp2C-"
      },
      "source": [
        "### 3.6.2 Mutual Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "PNDlVlGCp2C-"
      },
      "outputs": [],
      "source": [
        "def mutInfo(X, y):\n",
        "\n",
        "    if X.ndim > 2:\n",
        "        X = X.reshape(X.shape[0], -1)\n",
        "\n",
        "    importances = mutual_info_classif(X, y)\n",
        "    feat_importances = pd.Series(importances)\n",
        "\n",
        "    return feat_importances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I544wYswp2C-"
      },
      "source": [
        "### 3.6.3 Feature selection based on pearson corr. and mutual info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "9wMdoKEAp2C-"
      },
      "outputs": [],
      "source": [
        "#Feature selection based on Pearson Correlation and Inforamtion Gain\n",
        "def feat_selection(features_dict, frame_labels_dict):\n",
        "\n",
        "    cols_to_drop = []\n",
        "    features_dict_feat_sel = {}\n",
        "\n",
        "    #Convert dictionaries to numPy arrays of consecutive frames to do feature selection\n",
        "    X = np.concatenate(list(features_dict.values()))\n",
        "    y = np.concatenate(list(frame_labels_dict.values()))\n",
        "\n",
        "    corr_pairs = pearsonr(X)\n",
        "    feat_importances = mutInfo(X, y)\n",
        "\n",
        "    for pairs in corr_pairs:\n",
        "        cols_to_drop.append(pairs[0] if feat_importances[pairs[0]] < feat_importances[pairs[1]] else pairs[1])\n",
        "\n",
        "    #Make cols_to_drop a set to remove duplicates and then make it a list again\n",
        "    cols_to_drop = list(set(cols_to_drop))\n",
        "\n",
        "    #Put list values in ascending order\n",
        "    cols_to_drop.sort()\n",
        "\n",
        "    #Convert to numPy array\n",
        "    cols_to_drop = np.array(cols_to_drop)\n",
        "\n",
        "    #Drop selected features\n",
        "    for audio in features_dict:\n",
        "      features_dict_feat_sel[audio] = np.delete(features_dict[audio], cols_to_drop, axis=1)\n",
        "\n",
        "    return features_dict_feat_sel, cols_to_drop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e-yqjeXzv4l"
      },
      "source": [
        "# 4. Artificial NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "T2_Wvm-l0tyY"
      },
      "outputs": [],
      "source": [
        "#Define ANN model\n",
        "\n",
        "def ArtNet(X_train):\n",
        "\n",
        "    model = Sequential(name = 'ANN')\n",
        "    model.add(Dense(100, input_shape = (X_train.shape[1],), activation = 'relu', name='Hidden_Layer_1'))\n",
        "    model.add(Dropout(0.2, name='Dropout_Layer_1'))\n",
        "    model.add(Dense(6, activation = 'softmax', name='Output_Layer'))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer = 'adam',\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "            metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_8oCZtZi7Fl"
      },
      "source": [
        "# 5. CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "wNeQw1YKi9fu"
      },
      "outputs": [],
      "source": [
        "def convNet2D(X_train, kernel_size=4, pool_size=2, strides=(1, 1), dropout=0.2, dense_1=128, dense_2=64):\n",
        "\n",
        "    model = Sequential(name = 'CNN2D')\n",
        "    #First set of convolutional, MaxPolling and (Dropout) Layers\n",
        "    model.add(Conv2D(16, kernel_size, activation='relu',\n",
        "                     strides=strides, padding ='same', input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3]),\n",
        "                     name='Conv_layer_1'))\n",
        "    model.add(MaxPool2D(pool_size=pool_size,padding='same', name='MaxPool_layer_1'))\n",
        "    model.add(Dropout(dropout, name='Dropout_layer_1'))\n",
        "    #Second set of convolutional, MaxPolling and (Dropout) Layers\n",
        "    model.add(Conv2D(32, kernel_size, activation='relu',\n",
        "                     strides=strides, padding ='same', name='Conv_layer_2'))\n",
        "    model.add(MaxPool2D(pool_size=pool_size,padding='same', name='MaxPool_layer_2'))\n",
        "    model.add(Dropout(dropout, name='Dropout_layer_2'))\n",
        "     #Final set of convolutional, MaxPolling and (Dropout) Layers\n",
        "    model.add(Conv2D(64, kernel_size, activation='relu',\n",
        "                     strides=strides, padding='same', name='Conv_layer_3'))\n",
        "    model.add(MaxPool2D(pool_size=pool_size,padding='same', name='MaxPool_layer_3'))\n",
        "\n",
        "    model.add(Dropout(dropout, name='Dropout_layer_3'))\n",
        "    #Flattening to convert 2D array to 1D\n",
        "    model.add(Flatten(name='Flatten_layer'))\n",
        "    #Fully Connected Dense Layers\n",
        "    model.add(Dense(dense_1, activation='relu', name='Hidden_layer_1'))\n",
        "    model.add(Dense(dense_2, activation='relu', name='Hidden_layer_2'))\n",
        "    model.add(Dense(6, activation='softmax', name='Output_layer'))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    #Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=['accuracy'])   \n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UydewJzGMylc"
      },
      "source": [
        "# 6. Load data and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "EPC1c7YtJadX"
      },
      "outputs": [],
      "source": [
        "#Function to get final data for training and testing\n",
        "def get_dt(sig, augment: bool=True, mel_specs_comp: bool=False, feat_sel: bool=False,  stats: bool=False):\n",
        "\n",
        "  # Load features dictionary\n",
        "  if(mel_specs_comp):\n",
        "    features_dict = generate_mel_spec(sig)\n",
        "\n",
        "  else:\n",
        "    features_dict = feat_extr(sig, lp_order=12)\n",
        "\n",
        "  # Scale features of each game to [-1, 1]\n",
        "  for audio in features_dict:\n",
        "    scaler = StandardScaler()\n",
        "    features_dict[audio] = scaler.fit_transform(features_dict[audio])\n",
        "\n",
        "  #Dictionary with labels of each frame for every game\n",
        "  frame_labels_dict = new_frame_labelling(features_dict, df_dict)\n",
        "\n",
        "  # Feature selection\n",
        "  if(feat_sel):\n",
        "    features_dict, cols_to_drop = feat_selection(features_dict, frame_labels_dict)\n",
        "\n",
        "  #Create window dictionary containing windows for each game\n",
        "  windows_dict = create_win(features_dict)\n",
        "\n",
        "  #Create labels for each window of each game\n",
        "  windows_labels_dict = majVot_win_lab(windows_dict, frame_labels_dict)\n",
        "\n",
        "  #Choose signals for training and testing\n",
        "  signals_train = ['clips_lakers', 'okc_denver', 'portland_gsw', 'rockets_knicks', 'memphis_okc', 'bucks_toronto', 'portland_gsw']\n",
        "  signals_test = ['sixers_lakers', 'lakers_okc', 'gsw_cavs']\n",
        "\n",
        "\n",
        "  win_train_list = []\n",
        "  win_test_list = []\n",
        "  labels_train_list = []\n",
        "  labels_test_list = []\n",
        "\n",
        "  for audio in sig:\n",
        "    if audio in signals_train:\n",
        "      win_train_list.append(windows_dict[audio])\n",
        "      labels_train_list.append(windows_labels_dict[audio])\n",
        "    else:\n",
        "      win_test_list.append(windows_dict[audio])\n",
        "      labels_test_list.append(windows_labels_dict[audio])\n",
        "\n",
        "  #Concatenate signals and labels for training\n",
        "  win_train = np.concatenate(win_train_list, axis=0)\n",
        "  # labels_train = np.array(labels_train_list)\n",
        "  labels_train = np.concatenate(labels_train_list, axis=0)\n",
        "\n",
        "  win_test = np.concatenate(win_test_list, axis=0)\n",
        "  # labels_test = np.array(labels_test_list)\n",
        "  labels_test = np.concatenate(labels_test_list, axis=0)\n",
        "\n",
        "  # Augmentation check\n",
        "  if(augment):\n",
        "    aug_signals = aug(filtered_signals)\n",
        "    if(mel_specs_comp):\n",
        "      aug_feat_dict = generate_mel_spec(aug_signals)\n",
        "    else:\n",
        "      aug_feat_dict = feat_extr(aug_signals, lp_order=12)\n",
        "\n",
        "    for audio in aug_feat_dict:\n",
        "      scaler = StandardScaler()\n",
        "      aug_feat_dict[audio] = scaler.fit_transform(aug_feat_dict[audio])\n",
        "\n",
        "    aug_frame_labels_dict = new_frame_labelling(aug_feat_dict, df_dict)\n",
        "\n",
        "    if(feat_sel):\n",
        "        for audio in aug_feat_dict:\n",
        "          aug_feat_dict[audio] = np.delete(aug_feat_dict[audio], cols_to_drop, axis=1)\n",
        "\n",
        "\n",
        "    aug_windows_dict = create_win(aug_feat_dict)\n",
        "\n",
        "    #Create labels for each window of each game\n",
        "    aug_windows_labels_dict = majVot_win_lab(aug_windows_dict, aug_frame_labels_dict)\n",
        "\n",
        "    for audio in aug_feat_dict:\n",
        "       if audio in signals_train:\n",
        "        win_train_list.append(aug_windows_dict[audio])\n",
        "        labels_train_list.append(aug_windows_labels_dict[audio])\n",
        "\n",
        "    #Concatenate signals and labels for training\n",
        "    win_train = np.concatenate(win_train_list, axis=0)\n",
        "    # labels_train = np.array(labels_train_list)\n",
        "    labels_train = np.concatenate(labels_train_list, axis=0)\n",
        "\n",
        "\n",
        "  #if stats==True, use stats of windows on training and testing\n",
        "  if(stats):\n",
        "    win_train = win_stats(win_train)\n",
        "    win_test = win_stats(win_test)\n",
        "\n",
        "  return win_train,  labels_train, win_test, labels_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "U6zNpev9BB26"
      },
      "outputs": [],
      "source": [
        "#Function to load final model\n",
        "def load_model(X_train):\n",
        "\n",
        "  if conf.nn == 'cnn':\n",
        "    if X_train.ndim == 3:\n",
        "      model = convNet1D(X_train)\n",
        "    elif X_train.ndim == 4:\n",
        "      model = convNet2D(X_train)\n",
        "\n",
        "  elif conf.nn == 'ann' and X_train.ndim == 2:\n",
        "    model = ArtNet(X_train)\n",
        "\n",
        "  else:\n",
        "    raise Exception(\"Invalid data or Neural Network architecture\")\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K03H8djGOgE1"
      },
      "source": [
        "# 7. Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "LPq2Gr_9Fvjv"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "  X_train, y_train, X_test, y_test = get_dt(filtered_signals)\n",
        "\n",
        "  model = load_model(X_train)\n",
        "\n",
        "  print('\\n\\n\\n')\n",
        "  print('----------------------------------TRAINING PROCESS----------------------------------')\n",
        "\n",
        "  history = model.fit(X_train, y_train, epochs=10, batch_size=256, shuffle=True)\n",
        "\n",
        "  #Plot training loss\n",
        "  plt.figure(figsize=(12, 4))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(history.history['loss'], label='Training Loss')\n",
        "  plt.title('Training Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot training accuracy\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "  plt.title('Training Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  #Print confusion matrix to see values distribution on each class\n",
        "  print('\\n\\n\\n')\n",
        "  print('----------------------------------TESTING PROCESS----------------------------------')\n",
        "  y_pred_proba = model.predict(X_test)\n",
        "  y_pred = np.argmax(y_pred_proba, axis=1) #convert probability for each class into class prediction\n",
        "  acc = accuracy_score(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred, average='macro')\n",
        "  recall = recall_score(y_test, y_pred, average=None)\n",
        "  f1_macro = fbeta_score(y_test, y_pred, beta=1, average='macro')\n",
        "  print(\"Precision score: \" +str(precision)+ \"\\n Recall score: \"+str(recall)+ \"\\n Accuracy score:\"+str(acc)+\"\\n F1_macro score: \"+str(f1_macro))\n",
        "  confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  # class_labels = ['other', 'cheering', 'whistle', 'air_horn', 'boos']\n",
        "  class_labels = ['cheering', 'whistle', 'air_horn', 'speech', 'booing', 'other']\n",
        "\n",
        "  plt.figure(figsize=(8,6))\n",
        "  sns.heatmap(confusion_mat, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels, cmap='YlGnBu')\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.xlabel('Predicted Labels')\n",
        "  plt.ylabel('True Labels')\n",
        "  plt.show()\n",
        "\n",
        "  return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9piEH9SYJuMJ",
        "outputId": "d26d150a-e2bd-490d-fafa-5e5c2d736227"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  model, history = main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EpEa4DhKLkif",
        "LYk3ZCUYp2C0",
        "tHG5FMQzL-ID",
        "ryCmoHbKMG6Z",
        "wNpXeVH9p2C4",
        "Xxg5N3lSp2C5",
        "CYnIKA-jML6x",
        "HYPWqUKMMQLe",
        "AI6KY3I2MWzI",
        "-WJyNfLNPH23",
        "FntzKUpjPMbJ",
        "qkpMR9ZXp2C-",
        "I544wYswp2C-",
        "yp7MLTgwy4XR",
        "d_8oCZtZi7Fl",
        "IGiypbjPNhg8",
        "NHWl9ZT-NkiT"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
